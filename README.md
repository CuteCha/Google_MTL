[TOC]

# Google多任务模型

## MMoE

### MTL背景

> MTL(Multi-Task Learning)有很多形式：联合学习(joint learning)、自主学习(learning to learn)和带有辅助任务的学习(learning with auxiliary task，如ESMM系列)等等。一般来说，优化多个损失函数就等同于进行多任务学习(与单任务学习相反)。

> MLT 主要有两种形式，一种是基于参数的共享，另一种是基于约束的共享。
>
> - hard参数共享
>
>   参数共享的形式在基于神经网络的 MLT 中非常常见，其在所有任务中共享隐藏层并同时保留几个特定任务的输出层。
>
>   这种方式有助于降低过拟合风险，因为同时学习的任务越多，模型找到一个含有所有任务的表征就越困难，从而过拟合某个特定任务的可能性就越小，ESMM 就属于这种类型的 MLT。
>
> - soft参数共享
>
>   每个任务都有自己的参数和模型，最后通过对不同任务的参数之间的差异施加约束。比如可以使用L2进行正则、迹范数（trace norm）等。

> 为什么 MLT 有效呢？主要有以下几点原因：
>
> 1. 多任务一起学习时，会互相增加噪声，从而提高模型的泛化能力；
> 2. 多任务相关作用，逃离局部最优解；
> 3. 多任务共同作用模型的更新，增加错误反馈；
> 4. 降低了过拟合的风险；
> 5. 类似 ESMM，解决了样本偏差和数据稀疏问题，也可以用来解决冷启动问题。

### MTL适用条件

> MTL 的目标在于**通过利用包含在相关任务训练信号中特定领域的信息来提高泛化能力**。

> 什么是相关任务呢？有以下几个不严谨的解释：
>
> 1. 使用相同特征做判断的任务；
> 2. 任务的分类边界接近；
> 3. 预测同个个体属性的不同方面比预测不同个体属性的不同方面更相关；
> 4. 共同训练时能够提供帮助并不一定相关，因为加入噪声有时也可以增加泛化能力。

> 作者在论文中给出了多任务模型在相关性不同的数据集上的表现，证明：相关性越低，多任务学习的效果越差，如图：

img

> 在实际过程中，如何去识别不同任务之间的相关性也是非常难的。基于此，作者提出了 MMoE 框架，旨在构建一个兼容性更强的多任务学习框架。

### 模型

#### Shared-Bottom model

> ESMM 模型就是基于 shared-bottom 的多任务模型，这篇文章把该框架作为多任务模型的 baseline，其结构如下图所示：
>
> 所有任务共享底层网络，并同时保留几个特定任务的输出层。

img

#### One-gate MoE Layer

> One-gate MoE layer 是将隐藏层划分为多个专家(expert)子网，同时接入一个 Gate 网络，将各个子网的输出和输入信息进行组合，并将得到的结果进行相加。

img



#### Multi-gate MoE model

> One-gate MoE 能够实现不同数据多样化使用共享层，但针对不同任务而言，其使用的共享层是一致的。这种情况下，如果任务相关性较低，则会导致模型性能下降。所以，作者在此基础上提出了 MMoE 模型，为每个任务都设置了一个 Gate 网路，旨在使得不同任务和不同数据可以多样化的使用共享层，其模型结构如下：

img

> 这种情况下，每个 Gate 网络都可以根据不同任务来选择专家网络的子集，所以即使两个任务并不是十分相关，那么经过 Gate 后也可以得到不同的权重系数。此时，MMoE 可以充分利用部分 expert 网络的信息，近似于单个任务；而如果两个任务相关性高，那么 Gate 的权重分布相差会不大，会类似于一般的多任务学习。

### 实验结果

#### MTL模型在不同相关性任务下的loss分布

img

> 论文比较了不同 MLT 模型在不同相关性任务下的loss分布，其可以反应模型的鲁棒性，从图中可以看出：
>
> 1. shared-bottom模型的表现方差高于OMoE和MMoE；
> 2. 虽然在correlation=1的数据集下，OMoE和MMoE的表现接近，但是在correlation=0.5的数据集下，OMoE的表现有明显的下降，这也证明了MMoE的multi-gate structure能够有效的解决任务差异冲突带来的局部最小值；

#### 数据集的效果

> 论文比较了各个模型在两个数据集上的效果，如下：

img

#### Google推荐系统效果

> 论文比较了各个模型在Google推荐系统上的效果，如下：

img

> 此外，论文展示了gate在两个任务上的分布，satisfaction任务的label比engagement任务的label更加稀疏，所以satisfaction的gate分布更加偏向于单个expert。

img

### 代码

https://github.com/drawbridge/keras-mmoe

## MMoE在YouTube的应用







## MoSE

### 背景





